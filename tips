Git stuff:
git tag -a v1.0 -m "Version 1.0"
git describe --tags --long
git add . ; git commit -m "added versioning"
git push
https://gist.github.com/blackfalcon/8428401
https://github.com/git-tips/tips#git-aliases


#Exclude dir from tar
tar --exclude='./.git' -czvf datafabric_splunk.tgz datafabric_splunk/*
-----------------------


Docker stuff:
docker ps -a
docker images
-----------------------

HDFS stuff:
$HADOOP_HOME/sbin/start-dfs.sh
$HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/root/archive/splunkaccesscombine_archive
$HADOOP_HOME/bin/hdfs dfs -chmod -R 777 /user
$HADOOP_HOME/bin/hadoop fs -chown -R splunk:splunk hdfs://localhost:8020/user/
$HADOOP_HOME/bin/hdfs dfs -put /tmp/Hunkdata.json.gz /user/root/data/
$HADOOP_HOME/bin/hdfs dfs -put /tmp/avro_08_01_2017.avro /user/root/data/avro/20170801

Note: Every 60 minutes, the Splunk Archiver App on each indexer should execute the search “= | archivebuckets”. The “| archivebuckets forcerun=1” command will manually trigger bucket copying from the indexer to HDFS, similar to the following command:
-----------------------

Yarn stuff:
$HADOOP_HOME/sbin/stop-dfs.sh
$HADOOP_HOME/sbin/start-dfs.sh
----------------------

Kafka stuff:
#check if topic exist
RESULT=`$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper localhost:2181|grep $KAFKA_TOPIC`
Crete topic:
$KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic $KAFKA_TOPIC
Load data into topic
cat $SAMPLE_KAFKA_LOG | $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic $KAFKA_TOPIC



